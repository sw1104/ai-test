# Ai-Test

> A repository that does this and that for AI development

## Table of Contents
[chunk to chunk similarity](#chunk-to-chunk-similarity)  
[similarity king : 왕](#similarity-king--왕)  
[mac use mps for fine tuning](#mac-use-mps-for-fine-tuning)  
[original llm vs quantized llm](#original-llm-vs-quantized-llm)  


## chunk to chunk similarity

<img width="1373" alt="스크린샷 2024-07-27 오후 6 22 19" src="https://github.com/user-attachments/assets/d9d91fc3-320b-4301-a43c-eccaf22e5053">

## similarity king : 왕

<img width="434" alt="스크린샷 2024-07-27 오후 6 22 58" src="https://github.com/user-attachments/assets/162402f4-0785-40e4-88cb-7604d820ed5a">
<img width="434" alt="스크린샷 2024-07-27 오후 6 23 10" src="https://github.com/user-attachments/assets/ba57c6e6-91ba-4b96-a010-22c9da3689ff">

## mac use mps for fine tuning

<img width="383" alt="스크린샷 2024-07-27 오후 6 23 19" src="https://github.com/user-attachments/assets/be46aaf3-7e94-478e-926e-543c40e57840">

### fine tuning with mps on my mac

<img width="459" alt="스크린샷 2024-07-27 오후 6 26 01" src="https://github.com/user-attachments/assets/38782c8e-6f9f-4df4-8c38-314228e3ddfa">
</br>
...... ㅠㅠ 

## original llm vs quantized llm

### colab T4, L4 터져버림 A100 써야함..

<img width="142" alt="스크린샷 2024-07-27 오후 5 34 43" src="https://github.com/user-attachments/assets/0594779c-6279-4d0a-b2ed-d1d9610c9eae">

### 이건 쿠다로 텐서를 옮길 때. but pytorch는 양자화 연산을 쿠다에서 지원 안 함 -> 엔비디아에서 낸 tensorRT 라이브러리는 됨

<img width="158" alt="스크린샷 2024-07-27 오후 5 45 07" src="https://github.com/user-attachments/assets/78f48d32-f09b-4786-a83d-33dc3c8d8d8d">
</br></br>
<img width="583" alt="스크린샷 2024-07-27 오후 6 32 00" src="https://github.com/user-attachments/assets/38c99b73-73d9-47ab-8489-7a3dc239019e">

</br>
<div align="right">
  
[Table of Contents](#table-of-contents)

</div>
</br></br>
